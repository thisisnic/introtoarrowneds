---
logo: "images/logo.png"
execute:
  echo: true
format:
  revealjs: 
    theme: default
    slide-number: true  
footer: "[🔗 https://github.com/thisisnic/introtoarrowneds](https://github.com/thisisnic/introtoarrowneds)"
engine: knitr
editor: source
---

## Introduction to Arrow in R

NEDs Workshop (18th July 2024)

![](images/logo.png){fig-align="center" width="415" height="500"}

## Introductions

-   Nic Crane

Arrow contributor!

::: notes
Have participants introduce themselves in the chat
:::

## Welcome

Today we're going to cover:

-   Working with larger-than-memory datasets with Arrow
-   How to get the best performance with your tabular data
-   Where to find more information

## Workshop format

-   Slides available at <https://tinyurl.com/introtoarrowneds>
-   Follow-along coding
-   Time to ask questions

::: notes
-   feel free to ask questions as we go along!
:::

## Getting set up

![](images/newproj.png)

Repository URL: <https://github.com/thisisnic/introtoarrowneds>

## Dataset to follow along with

```{r}
#| label: get-data
#| eval: false
options(timeout = 1800)
download.file(
  url = "https://r4ds.s3.us-west-2.amazonaws.com/seattle-library-checkouts.csv",
  destfile = "./data/seattle-library-checkouts.csv"
)
```

::: notes
-   dataset from R For Data Science
-   run this from within the project you've got set up
-   first line of code increase the download timeout to 30 mins - essential when downloading larger datasets
-   if you don't have the data yet, download it now - next section talking about arrow
:::


# Part 1 - Arrow

::: notes
-   any questions before we get started?
-   this section will cover a bit of background info about Apache Arrow
:::

## What is Apache Arrow?

::: columns
::: {.column width="50%"}
> A multi-language toolbox for accelerated data interchange and in-memory processing
:::

::: {.column width="50%"}
> Arrow is designed to both improve the performance of analytical algorithms and the efficiency of moving data from one system or programming language to another
:::
:::

::: {style="font-size: 70%;"}
<https://arrow.apache.org/overview/>
:::

::: notes
-   TODO: add main points want to hit here
:::

## Apache Arrow Specification

In-memory [columnar format]{style="background-color: #F3D1FF;"}: a [standardized, language-agnostic specification]{style="background-color: #FFDCB7;"} for representing structured, [table-like datasets]{style="background-color: #DCFFC9;"} in-memory.

<br>

![](images/arrow-rectangle.png){.absolute left="200"}

::: notes
-   brief overview of these points but coming back to later
:::

## A Multi-Language Toolbox

![](images/arrow-libraries-structure.png)

## Accelerated Data Interchange

![](images/data-interchange-with-arrow.png)

::: notes
-   standardisation means multiple things speaking Arrow prevents copying back and forth
:::

## Accelerated In-Memory Processing

Arrow's Columnar Format is Fast

![](images/columnar-fast.png){.absolute top="120" left="200" height="550"}

::: notes
-   intro the toy dataset
-   memory buffers are a 1-dimension structure, not like a 2D table/data.frame
-   walk through this in the 2 diagrams
-   analytic workflows typically have filtering, grouping by columns etc; give examples
-   faster to scan adjacent areas than picking through it all + taking advantage of vectorization available on modern processes speeds up faster
:::

## arrow 📦

<br>

![](images/arrow-r-pkg-highlights.png){.absolute top="0" left="300" width="700" height="900"}

## arrow 📦

![](images/arrow-read-write-updated.png)

::: notes
-   different types of objects
-   different file formats
-   different storage locations
:::

# Part 2 - Working with Arrow Datasets

::: notes
-   any questions so far?
-   maybe a poll - who's worked with dplyr/arrow/parquet before?
:::

## Seattle Checkouts - Big CSV

![](images/seattle-checkouts.png){.absolute top="120" left="200" height="550"}

::: {style="font-size: 70%;"}
<https://data.seattle.gov/Community/Checkouts-by-Title/tmmm-ytt6/about_data>
:::

## Dataset contents

![](images/datapreview.png){height="550"}

::: notes
-   LIVE CODING
    -   path to the data depending on where you've downloaded it
    -   impact of data size on performance
    -   walk through subcomponents of the output when print the `seattle_csv` object
        -   explicitly mention the term "schema"
        -   string and character are direct equivalents
        -   why we have e.g. 64-bit integers
        -   arrow automatically handles the conversion between R and Arrow data types
    -   these types have been guessed from first 1MB of rows (can't say how many as varies with ncol)
    -   can anyone spot something odd here?
    -   what is an ISBN; what is the null type?
    -   pulling out the schema
    -   updating the schema
    -   using the e.g. `string()` functions to create different data types and where in the docs?
    -   show *both* schema updating and `col_types`
    -   checking out the new schema
    -   how often is this necessary? Often not. Good practice. Only important with CSVs not Parquet.
    -   run `glimpse()` but remember it'll take a moment
        -   TODO: find something to talk about here or don't use glimpse!
        -   42 millions rows
        -   UsageClass column data
        -   ISBN column data
        -   PublicationYear + why it's a string
-   Recap this section!
-   Questions?
:::

## How big is the dataset?

```{r}
#| label: setup
#| echo: false
#| output: false
library(arrow)
library(dplyr)
```

```{r}
#| label: check-file-size
library(arrow)
library(dplyr)
file.size("./data/seattle-library-checkouts.csv") / 10 **9
```

## Opening in Arrow

```{r}
#| label: open-dataset
seattle_csv <- open_dataset(
  sources = "./data/seattle-library-checkouts.csv", 
  format = "csv"
)
```

## How many rows of data?

```{r}
#| label: nrow-dataset
nrow(seattle_csv)
```

## Extract schema

```{r}
#| label: extract-schema
schema(seattle_csv)
```

## Arrow Data Types

Arrow has a rich data type system, including direct analogs of many R data types

-   `<dbl>` == `<double>`
-   `<chr>` == `<string>` or `<utf8>`
-   `<int>` == `<int32>`

<br>

<https://arrow.apache.org/docs/r/articles/data_types.html>

## Parsing the Metadata

<br>

Arrow scans 👀 1MB of the file(s) to impute or "guess" the data types

::: {style="font-size: 80%; margin-top: 200px;"}
📚 arrow vs readr blog post: <https://thisisnic.github.io/2022/11/21/type-inference-in-readr-and-arrow/>
:::

## Parsers Are Not Always Right

```{r}
#| label: seattle-schema-again
schema(seattle_csv)
```

![](images/data-dict.png){.absolute top="200" left="330" width="700"}

::: notes
International Standard Book Number (ISBN) is a 13-digit number that uniquely identifies books and book-like products published internationally.

Data Dictionaries, metadata in data catalogues should provide this info.
:::

## Let's Control the Schema

```{r}
#| label: open-dataset-schema
seattle_csv <- open_dataset(
  sources = "./data/seattle-library-checkouts.csv", 
  col_types = schema(ISBN = string()),
  format = "csv"
)

schema(seattle_csv)
```

# Part 3 - Data Manipulation with Arrow

::: notes
-   \<Up to here was about 28 mins in first practice run (no time for questions)\>
-   Question - how many people here have used dbplyr to connect to a database in R?
:::

## Arrow dplyr backend

![](images/dplyr-backend.png)

::: notes
LIVE CODING - Data contains book, ebooks, things which aren't book - Do *not* call collect() after first query - talk about lazy eval and show the query - endsWith to ends_with; this is actually an arrow C++ lib func - Don't want to pull all into memory as it's a lot of data; preview using head - Now look at the outputs of that - Next: How many books and ebooks were checked out each year?
- Don't need to call head() to preview as the data returned is just a row for each year - Run again with a timer set, then walk through results - Walk through the data, drop in 2020 - pandemic?
- Not bad, it can be faster and this is what we'll talk about in part 3 RECAP - Any questions?
:::

## Querying the data - new column: is this a book?

```{r}
#| label: query
seattle_csv |>
  mutate(IsBook = endsWith(MaterialType, "BOOK")) |>
  select(MaterialType, IsBook)
```

Nothing is pulled into memory yet!

## Preview the query

```{r}
#| label: preview-query
#| code-line-numbers: "|2,5"
seattle_csv |>
  head(20) |>
  mutate(IsBook = endsWith(MaterialType, "BOOK")) |>
  select(MaterialType, IsBook) |>
  collect()
```

## How many books were checked out each year?

```{r}
#| label: seattle-data-manip
seattle_csv |>
  filter(endsWith(MaterialType, "BOOK")) |>
  group_by(CheckoutYear) |>
  summarise(Checkouts = sum(Checkouts)) |>
  arrange(CheckoutYear) |> 
  collect()
```

## How long did it take?

```{r}
#| label: seattle-dplyr-timed
#| code-line-numbers: "6"
seattle_csv |>
  filter(endsWith(MaterialType, "BOOK")) |>
  group_by(CheckoutYear) |>
  summarise(Checkouts = sum(Checkouts)) |>
  arrange(CheckoutYear) |> 
  collect() |>
  system.time()
```

42 million rows -- not bad, but could be faster....

# Part 4 - Engineering the Data

## .NORM Files

![](images/norm_normal_file_format_2x.png){.absolute top="0" left="400"}

<br>

::: {style="font-size: 70%;"}
<https://xkcd.com/2116/>
:::

## File Format: Apache Parquet

![](images/apache-parquet.png){.absolute top="100" left="200" width="700"}

::: {style="font-size: 60%; margin-top: 450px;"}
<https://parquet.apache.org/>
:::

::: notes
-   While CSVs widely used common format, Parquet is a popular format for bigger data
-   optimised for analytics workflows
:::

## Parquet Files: "row-chunked"

![](images/parquet-chunking.png)

## Parquet Files: "row-chunked & column-oriented"

![](images/parquet-columnar.png)


## Parquet

-   compression and encoding == usually much smaller than equivalent CSV file, less data to move from disk to memory
-   rich type system & stores the schema along with the data == more robust pipelines
-   "row-chunked & column-oriented" == work on different parts of the file at the same time or skip some chunks all together, better performance than row-by-row

## Writing to Parquet

```{r}
#| label: seattle-write-parquet-setup
#| output: false
#| echo: false
seattle_parquet_dir <- "./data/seattle-library-checkouts-parquet"
```

```{r}
#| label: seattle-write-parquet-single
#| eval: false
seattle_parquet_dir <- "./data/seattle-library-checkouts-parquet"

seattle_csv |>
  write_dataset(path = seattle_parquet_dir, format = "parquet")
```

## Storage: Parquet vs CSV

```{r}
#| label: seattle-single-parquet-size
file <- list.files(seattle_parquet_dir,
                   recursive = TRUE,
                   full.names = TRUE)

file.size(file) / 10 ** 9
```

<br>

Parquet about half the size of the CSV file on-disk 💾

## 4.5GB Parquet file + arrow + dplyr

```{r}
#| label: seattle-single-parquet-dplyr-timed
open_dataset(seattle_parquet_dir, 
             format = "parquet") |>
  filter(endsWith(MaterialType, "BOOK")) |>
  group_by(CheckoutYear) |>
  summarise(Checkouts = sum(Checkouts)) |>
  arrange(CheckoutYear) |> 
  collect() |>
  system.time()
```

<br>

42 million rows -- much better!
But could be *even* faster....

## File Storage: Partitioning

Dividing data into smaller pieces, making it more easily accessible and manageable

![](images/partitioning.png){fig-align="right"}

::: notes
also called multi-files or sometimes shards
:::

## Poll: Partitioning?

Have you partitioned your data or used partitioned data before today?

-   1️⃣ Yes
-   2️⃣ No
-   3️⃣ Not sure, the data engineers sort that out!

## Rewriting the Data Again

```{r}
#| label: seattle-write-partitioned
#| eval: false
seattle_parquet_part <- "./data/seattle-library-checkouts"

seattle_csv |>
  group_by(CheckoutYear) |>
  write_dataset(path = seattle_parquet_part,
                format = "parquet")
```

## What Did We "Engineer"?

```{r}
#| label: seattle-partitioned-sizes
seattle_parquet_part <- "./data/seattle-library-checkouts"

sizes <- tibble(
  files = list.files(seattle_parquet_part,
                     recursive = TRUE),
  size_GB = round(file.size(file.path(seattle_parquet_part, files)) / 10**9, 3)
)

sizes
```

## 4.5GB partitioned Parquet files + arrow + dplyr

```{r}
#| label: seattle-partitioned-dplyr-timed
seattle_parquet_part <- "./data/seattle-library-checkouts"

open_dataset(seattle_parquet_part,
             format = "parquet") |>
  filter(endsWith(MaterialType, "BOOK")) |>
  group_by(CheckoutYear) |>
  summarise(Checkouts = sum(Checkouts)) |>
  arrange(CheckoutYear) |>
  collect() |>
  system.time()
```

<br>

42 million rows -- not too shabby!

## Art & Science of Partitioning

<br>

-   avoid files \< 20MB and \> 2GB
-   avoid \> 10,000 files (🤯)
-   partition on variables used in `filter()`

::: notes
-   guidelines not rules, results vary
-   experiment
-   arrow suggests avoid files smaller than 20MB and larger than 2GB
-   avoid partitions that produce more than 10,000 files (Arrow reads the metadata of each file)
-   partition by variables that you filter by, allows arrow to only read relevant files
:::

## Performance Review: Single CSV

How long does it take to calculate the number of books checked out in each month of 2021?

<br>

```{r}
#| label: seattle-single-csv-dplyr-timed
open_dataset(sources = "./data/seattle-library-checkouts.csv",
             format = "csv") |>
  filter(CheckoutYear == 2021, endsWith(MaterialType, "BOOK")) |>
  group_by(CheckoutMonth) |>
  summarize(TotalCheckouts = sum(Checkouts)) |>
  arrange(desc(CheckoutMonth)) |>
  collect() |>
  system.time()
```

## Performance Review: Partitioned Parquet

How long does it take to calculate the number of books checked out in each month of 2021?

<br>

```{r}
#| label: seattle-parquet-partitioned-dplyr-timed
open_dataset("./data/seattle-library-checkouts",
             format = "parquet") |>
  filter(CheckoutYear == 2021, endsWith(MaterialType, "BOOK")) |>
  group_by(CheckoutMonth) |>
  summarize(TotalCheckouts = sum(Checkouts)) |>
  arrange(desc(CheckoutMonth)) |>
  collect() |>
  system.time()
```

## Engineering Data Tips for Improved Storage & Performance

<br>

-   use Parquet over CSV if possible
-   consider partitioning, experiment to get an appropriate partition design 🗂️
-   watch your schemas 👀

# Part 5 - More Resources

## Arrow docs

<https://arrow.apache.org/docs/r/>

![](images/docs.png)

## R for Data Science (2e)

::: columns
::: {.column width="50%"}
![](images/r4ds-cover.jpg){.absolute top="100" width="400"}
:::

::: {.column width="50%"}
<br>

[Chapter 23: Arrow](https://r4ds.hadley.nz/arrow.html)

<br>

<https://r4ds.hadley.nz/>
:::
:::

## Scaling Up with R and Arrow

::: columns
::: {.column width="50%"}
![](images/dummybookcover.png)
:::

::: {.column width="50%"}
Currently being written - preview available online soon!
:::
:::
